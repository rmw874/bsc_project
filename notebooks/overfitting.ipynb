{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torch.nn.functional import relu, softmax, one_hot\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data preperation (preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE TO SELF: The images should be of consistent dimensions for better results. This notebook specifically overfits one case. Although UNET is fully convolutional, the dimensions must be divisible by the downsampling operations (dimensions%2^n = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('../data/raw/images/8013620831-0043.jpg-t.jpg')\n",
    "mask = cv2.imread('../data/raw/masks/8013620831-0043.jpg-t.png')\n",
    "img = img / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(mask)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# red = rows\n",
    "# green = columns (we care about this)\n",
    "green_channel = mask[:, :, 1] # 1 for green since (R, G, B)\n",
    "class_mask = np.zeros_like(green_channel)\n",
    "\n",
    "class_mask[green_channel == 1] = 0  # Year \n",
    "class_mask[green_channel == 2] = 1  # Date\n",
    "class_mask[green_channel == 3] = 2  # Latitude\n",
    "class_mask[green_channel == 4] = 3  # Longitude\n",
    "class_mask[green_channel == 5] = 4  # Water Temperature\n",
    "class_mask[green_channel == 255] = 5  # Background\n",
    "\n",
    "mask_class = torch.tensor(class_mask, dtype=torch.long)\n",
    "one_hot_mask = one_hot(mask_class, num_classes=6)\n",
    "\n",
    "print(one_hot_mask.shape)  # [height, width, num_classes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if we have correctly one-hot encoded the classes (being columns and background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i in range(0, 6):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.imshow(one_hot_mask[..., i])\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.e11 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.bn11 = nn.BatchNorm2d(64)\n",
    "        self.e12 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn12 = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.e21 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn21 = nn.BatchNorm2d(128)\n",
    "        self.e22 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn22 = nn.BatchNorm2d(128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.e31 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn31 = nn.BatchNorm2d(256)\n",
    "        self.e32 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.bn32 = nn.BatchNorm2d(256)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.e41 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.bn41 = nn.BatchNorm2d(512)\n",
    "        self.e42 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn42 = nn.BatchNorm2d(512)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.e51 = nn.Conv2d(512, 1024, kernel_size=3, padding=1)\n",
    "        self.bn51 = nn.BatchNorm2d(1024)\n",
    "        self.e52 = nn.Conv2d(1024, 1024, kernel_size=3, padding=1)\n",
    "        self.bn52 = nn.BatchNorm2d(1024)\n",
    "\n",
    "        # Decoder\n",
    "        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.d11 = nn.Conv2d(1024, 512, kernel_size=3, padding=1)\n",
    "        self.dbn11 = nn.BatchNorm2d(512)\n",
    "        self.d12 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.dbn12 = nn.BatchNorm2d(512)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.d21 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.dbn21 = nn.BatchNorm2d(256)\n",
    "        self.d22 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.dbn22 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.d31 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.dbn31 = nn.BatchNorm2d(128)\n",
    "        self.d32 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.dbn32 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.d41 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.dbn41 = nn.BatchNorm2d(64)\n",
    "        self.d42 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.dbn42 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.outconv = nn.Conv2d(64, n_class, kernel_size=1)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        xe11 = relu(self.bn11(self.e11(x)))\n",
    "        xe12 = relu(self.bn12(self.e12(xe11)))\n",
    "        xp1 = self.pool1(xe12)\n",
    "\n",
    "        xe21 = relu(self.bn21(self.e21(xp1)))\n",
    "        xe22 = relu(self.bn22(self.e22(xe21)))\n",
    "        xp2 = self.pool2(xe22)\n",
    "\n",
    "        xe31 = relu(self.bn31(self.e31(xp2)))\n",
    "        xe32 = relu(self.bn32(self.e32(xe31)))\n",
    "        xp3 = self.pool3(xe32)\n",
    "\n",
    "        xe41 = relu(self.bn41(self.e41(xp3)))\n",
    "        xe42 = relu(self.bn42(self.e42(xe41)))\n",
    "        xp4 = self.pool4(xe42)\n",
    "\n",
    "        xe51 = relu(self.bn51(self.e51(xp4)))\n",
    "        xe52 = relu(self.bn52(self.e52(xe51)))\n",
    "        \n",
    "        # Decoder\n",
    "        xu1 = self.upconv1(xe52)\n",
    "        xu11 = torch.cat([xu1, xe42], dim=1)\n",
    "        xd11 = relu(self.dbn11(self.d11(xu11)))\n",
    "        xd12 = relu(self.dbn12(self.d12(xd11)))\n",
    "\n",
    "        xu2 = self.upconv2(xd12)\n",
    "        xu22 = torch.cat([xu2, xe32], dim=1)\n",
    "        xd21 = relu(self.dbn21(self.d21(xu22)))\n",
    "        xd22 = relu(self.dbn22(self.d22(xd21)))\n",
    "\n",
    "        xu3 = self.upconv3(xd22)\n",
    "        xu33 = torch.cat([xu3, xe22], dim=1)\n",
    "        xd31 = relu(self.dbn31(self.d31(xu33)))\n",
    "        xd32 = relu(self.dbn32(self.d32(xd31)))\n",
    "\n",
    "        xu4 = self.upconv4(xd32)\n",
    "        xu44 = torch.cat([xu4, xe12], dim=1)\n",
    "        xd41 = relu(self.dbn41(self.d41(xu44)))\n",
    "        xd42 = relu(self.dbn42(self.d42(xd41)))\n",
    "\n",
    "        out = self.outconv(xd42)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMS\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-4\n",
    "N_CLASSES = 6\n",
    "TARGET_SIZE = (3200, 2496)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(N_CLASSES).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_resized = cv2.resize(img, TARGET_SIZE, interpolation=cv2.INTER_AREA)\n",
    "img_tensor = torch.from_numpy(img_resized).permute(2, 0, 1).float().unsqueeze(0).to(device)\n",
    "\n",
    "mask_resized = cv2.resize(class_mask, TARGET_SIZE, interpolation=cv2.INTER_NEAREST)\n",
    "mask_class_resized = torch.tensor(mask_resized, dtype=torch.long)\n",
    "one_hot_mask_resized = one_hot(mask_class_resized, num_classes=6)\n",
    "mask_tensor = one_hot_mask_resized.permute(2, 0, 1).float().unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = torch.cuda.amp.GradScaler()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass with mixed precision\n",
    "    with torch.cuda.amp.autocast():\n",
    "        outputs = model(img_tensor)\n",
    "        loss = criterion(outputs, mask_tensor)\n",
    "    \n",
    "    # Backward pass with gradient scaling\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    \n",
    "    # print text and visualization\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        clear_output(wait=True)\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            pred = outputs.squeeze(0).cpu()\n",
    "            pred = torch.argmax(pred, dim=0).numpy()\n",
    "            \n",
    "            plt.figure(figsize=(15, 5))\n",
    "            \n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.imshow(img_resized)\n",
    "            plt.title('Original Image (Resized)')\n",
    "            \n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.imshow(mask_resized)\n",
    "            plt.title('Ground Truth (Resized)')\n",
    "            \n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.imshow(pred)\n",
    "            plt.title(f'Prediction (Epoch {epoch+1}), Loss: {loss.item():.4f}')\n",
    "            \n",
    "            plt.suptitle(f'Loss: {loss.item():.4f}', fontsize=24)\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "torch.save(model.state_dict(), 'overfitted_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv_3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
